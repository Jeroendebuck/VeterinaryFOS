name: Nightly ETL + dbt + Pages Deploy

on:
  schedule:
    - cron: '17 4 * * *'   # 04:17 UTC daily
  workflow_dispatch: {}
  push:
    branches: [ main ]
    paths:
      - 'etl/**'
      - 'models/**'
      - 'seeds/**'
      - 'dbt_project.yml'
      - 'profiles.yml'
      - 'requirements.txt'
      - '.github/workflows/etl.yml'
      - 'index.html'
      - 'styles.css'
      - 'app.js'

permissions:
  contents: read
  pages: write
  id-token: write

concurrency:
  group: pages-${{ github.ref }}
  cancel-in-progress: true

env:
  # Secrets: Settings → Secrets and variables → Actions → New repository secret
  OPENALEX_MAILTO: ${{ secrets.OPENALEX_MAILTO }}
  OPENALEX_API_KEY: ${{ secrets.OPENALEX_API_KEY }}
  OPENALEX_RATE_SECONDS: '0.25'
  OPENALEX_START_DATE: '2015-01-01'

jobs:
  build:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Fetch OpenAlex for roster
        run: |
          python etl/fetch_openalex.py

      - name: (Optional) Convert vet taxonomy YAML → CSV seed
        run: |
          if [ -f "data/vet_taxonomy.yaml" ]; then
            python etl/taxonomy_yaml_to_csv.py
          else
            echo "No data/vet_taxonomy.yaml present; skipping"
          fi

      - name: dbt seed + run
        run: |
          dbt deps || true
          dbt seed --profiles-dir . --project-dir . --full-refresh
          dbt run  --profiles-dir . --project-dir .

      - name: Build site bundle (HTML/CSS/JS + exported CSVs)
        run: |
          set -euo pipefail
          mkdir -p site site/exports
      
          # 1) copy static files from repo root
          cp -v index.html styles.css app.js site/
      
          # (optional) copy any images if present (safe when none exist)
          shopt -s nullglob
          for f in *.png *.jpg *.jpeg *.svg *.ico; do cp -v "$f" site/ || true; done
          shopt -u nullglob
      
          # 2) export CSVs directly into site/exports (preferred)
          DASHBOARD_OUT_DIR=site/exports python etl/export_for_dashboard.py || true
      
          # 3) fallback: only copy if at least one file matches
          if compgen -G "exports/*.csv" > /dev/null; then
            cp -v exports/*.csv site/exports/
          fi
      
          echo "Bundle contents:"
          find site -maxdepth 2 -type f -print


      - name: Upload site as Pages artifact
        uses: actions/upload-pages-artifact@v3
        with:
          path: site

  deploy:
    environment:
      name: github-pages
    runs-on: ubuntu-latest
    needs: build
    permissions:
      pages: write
      id-token: write
    steps:
      - name: Deploy to GitHub Pages
        id: deployment
        uses: actions/deploy-pages@v4
